{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data/11-785hw2p2-s20/\"\n",
    "MODEL_PATH = \"model/\"\n",
    "SUBMISSION_PATH = \"submission/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torchvision   \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sys\n",
    "import pandas as pd\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = torchvision.transforms.Compose([\n",
    "        #torchvision.transforms.ToPILImage(),\n",
    "        #torchvision.transforms.RandomResizedCrop(224),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder(root=DATA_PATH+'train_data/medium', transform=data_transforms)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,pin_memory=True)\n",
    "dev_dataset = torchvision.datasets.ImageFolder(root=DATA_PATH+'validation_classification/medium', \n",
    "                                               transform=data_transforms)\n",
    "dev_dataloader = torch.utils.data.DataLoader(dev_dataset, batch_size=batch_size, \n",
    "                                             shuffle=False, num_workers=num_workers,pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(822155, 2300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4600, 2300)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_dataset), len(dev_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_class = {v: k for k, v in train_dataset.class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs, classes = train_dataset[0]\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "imshow(out, title=idx_to_class[classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerificationDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with picture pairs\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.verification_frame = pd.read_csv(csv_file, sep = \" \", header = None)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.verification_frame.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img1_name = os.path.join(self.root_dir,\n",
    "                                self.verification_frame.iloc[idx, 0])\n",
    "        img2_name = os.path.join(self.root_dir,\n",
    "                                self.verification_frame.iloc[idx, 1])\n",
    "        \n",
    "        to_pil = torchvision.transforms.Compose([torchvision.transforms.ToPILImage()])\n",
    "        image1 = to_pil(plt.imread(img1_name))\n",
    "        image2 = to_pil(plt.imread(img2_name))\n",
    "        if self.transform:\n",
    "            image1 = self.transform(image1)\n",
    "            image2 = self.transform(image2)\n",
    "        if len(self.verification_frame.columns) == 3:\n",
    "            target = self.verification_frame.iloc[idx, 2].astype('int32')\n",
    "            return image1, image2, target\n",
    "        else:\n",
    "            return image1, image2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_dataset = VerificationDataset(DATA_PATH+\"validation_trials_verification.txt\",DATA_PATH+\"validation_verification\",data_transforms)\n",
    "verification_dataloader = DataLoader(verification_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "0\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "print (verification_dataset[0][0].shape)\n",
    "print (verification_dataset[0][2])\n",
    "print (len(verification_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following piece of code for Center Loss has been pulled and \n",
    "#modified based on the code from the GitHub Repo: https://github.com/KaiyangZhou/pytorch-center-loss\n",
    "#Reference: Wen et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.\n",
    "\n",
    "class CenterLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        feat_dim (int): feature dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, feat_dim):\n",
    "        super(CenterLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feat_dim = feat_dim\n",
    "        \n",
    "        self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: feature matrix with shape (batch_size, feat_dim).\n",
    "            labels: ground truth labels with shape (batch_size).\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
    "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
    "        distmat.addmm_(1, -2, x, self.centers.t())\n",
    "\n",
    "        classes = torch.arange(self.num_classes).long().cuda()\n",
    "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
    "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
    "\n",
    "        dist = []\n",
    "        for i in range(batch_size):\n",
    "            value = distmat[i][mask[i]]\n",
    "            value = value.clamp(min=1e-12, max=1e+12) # for numerical stability\n",
    "            dist.append(value)\n",
    "        dist = torch.cat(dist)\n",
    "        loss = dist.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor from recitation 6 and https://github.com/hysts/pytorch_resnet/blob/master/resnet.py\n",
    "\n",
    "class BottleneckBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "        self.expansion = 4\n",
    "        bottleneck_channels = out_channels // self.expansion\n",
    "        self.in_channels, self.out_channels = in_channels, out_channels\n",
    "        self.conv1 = nn.Conv2d(in_channels,bottleneck_channels,kernel_size=1,stride=1,padding=0,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(bottleneck_channels)\n",
    "        self.conv2 = nn.Conv2d(bottleneck_channels,bottleneck_channels,kernel_size=3,stride=stride,padding=1,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n",
    "        self.conv3 = nn.Conv2d(bottleneck_channels,out_channels,kernel_size=1,stride=1,padding=0,bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride,padding=0,bias=False),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
    "        y = F.relu(self.bn2(self.conv2(y)), inplace=True)\n",
    "        y = self.bn3(self.conv3(y))\n",
    "        y += self.shortcut(x)\n",
    "        y = F.relu(y, inplace=True)\n",
    "        return y\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor from recitation 6 and https://github.com/hysts/pytorch_resnet/blob/master/resnet.py\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, num_feats, num_classes):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        self.layers.append(nn.Conv2d(in_channels=num_feats, out_channels=64, kernel_size=3, padding = 1, stride=1, bias=False))\n",
    "        self.layers.append(nn.BatchNorm2d(64))\n",
    "        self.layers.append(nn.ReLU(inplace=True))\n",
    "        self.layers.append(nn.MaxPool2d(3, stride=2, padding=1))\n",
    "\n",
    "        in_channels = 64\n",
    "        for out_channels, block_count in zip([128,256,512,1024],[2,3,5,3]):\n",
    "            for i in range(block_count):\n",
    "                self.layers.append(BottleneckBlock(in_channels, out_channels))\n",
    "                in_channels = out_channels\n",
    "\n",
    "        self.layers.append(nn.AdaptiveAvgPool2d((1,1)))\n",
    "        self.layers.append(nn.Flatten())\n",
    "        self.layers.append(nn.Linear(1024, 4096))\n",
    "        self.layers.append(nn.BatchNorm1d(4096))\n",
    "        self.layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "        self.linear_label = nn.Linear(4096, num_classes, bias=False)\n",
    "    \n",
    "    def forward(self, x, evalMode=False):\n",
    "        features = self.layers(x)\n",
    "          \n",
    "        label_output = self.linear_label(features)\n",
    "        #label_output = label_output/torch.norm(self.linear_label.weight, dim=1)\n",
    "        \n",
    "        return features, label_output\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "    elif type(m) == nn.BatchNorm2d:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (layers): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): BottleneckBlock(\n",
      "      (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): BottleneckBlock(\n",
      "      (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (6): BottleneckBlock(\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): BottleneckBlock(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (8): BottleneckBlock(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (9): BottleneckBlock(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): BottleneckBlock(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (11): BottleneckBlock(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (12): BottleneckBlock(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (13): BottleneckBlock(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (14): BottleneckBlock(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): BottleneckBlock(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (16): BottleneckBlock(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (17): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (18): Flatten()\n",
      "    (19): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (20): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (21): ReLU(inplace=True)\n",
      "  )\n",
      "  (linear_label): Linear(in_features=4096, out_features=2300, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "feat_dim = 4096\n",
    "model = Network(3, len(train_dataset.classes))\n",
    "#model.apply(init_weights)\n",
    "model.load_state_dict(torch.load(MODEL_PATH+'model_1583478712.pt'))\n",
    "model.cuda()\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_label = nn.CrossEntropyLoss()\n",
    "criterion_closs = CenterLoss(len(train_dataset.classes), feat_dim)\n",
    "\n",
    "optimizer_label = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
    "scheduler_label = torch.optim.lr_scheduler.StepLR(optimizer_label, step_size=1, gamma=0.1)\n",
    "optimizer_closs = torch.optim.SGD(criterion_closs.parameters(), lr=0.5)\n",
    "\n",
    "closs_weight = 0.0001\n",
    "numEpochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_closs(model, data_loader, classification_test_loader, verification_test_loader, task='Verification'):\n",
    "    best_model = None\n",
    "    lowest_val_acc = None\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(numEpochs):\n",
    "        avg_loss = 0.0\n",
    "        for batch_num, (feats, labels) in tqdm(enumerate(data_loader)):\n",
    "            feats, labels = feats.cuda(), labels.cuda()\n",
    "            \n",
    "            optimizer_label.zero_grad()\n",
    "            optimizer_closs.zero_grad()\n",
    "            \n",
    "            feature, outputs = model(feats)\n",
    "\n",
    "            l_loss = criterion_label(outputs, labels.long())\n",
    "            c_loss = criterion_closs(feature, labels.long())\n",
    "            loss = l_loss + closs_weight * c_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer_label.step()\n",
    "            # by doing so, weight_cent would not impact on the learning of centers\n",
    "            for param in criterion_closs.parameters():\n",
    "                param.grad.data *= (1. / closs_weight)\n",
    "            optimizer_closs.step()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            if batch_num % 100 == 99:\n",
    "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/100))\n",
    "                avg_loss = 0.0    \n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "            del loss\n",
    "            \n",
    "        scheduler_label.step()\n",
    "        #scheduler_closs.step()\n",
    "            \n",
    "        val_loss, val_acc = test_classify_closs(model, classification_test_loader)\n",
    "        print('Val Loss: {:.4f}\\tVal Accuracy: {:.4f}'.format(val_loss, val_acc))\n",
    "        val_auc = test_verify(model, verification_test_loader)\n",
    "        print('Val Auc: {:.4f}'.format(val_auc))\n",
    "        torch.save(model.state_dict(), MODEL_PATH+'Checkpoint/closs_model_%d_%d.pt'%(int(time.time()), epoch+1))\n",
    "        if ((best_model == None) or (val_acc < lowest_acc)):\n",
    "            best_model = model\n",
    "            lowest_acc = val_acc\n",
    "    torch.save(best_model.state_dict(), MODEL_PATH+'closs_model_%d.pt'%int(time.time()))\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def test_classify_closs(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    accuracy = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_num, (feats, labels) in tqdm(enumerate(test_loader)):\n",
    "            feats, labels = feats.cuda(), labels.cuda()\n",
    "            feature, outputs = model(feats)\n",
    "\n",
    "            _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
    "            pred_labels = pred_labels.view(-1)\n",
    "\n",
    "            l_loss = criterion_label(outputs, labels.long())\n",
    "            c_loss = criterion_closs(feature, labels.long())\n",
    "            loss = l_loss + closs_weight * c_loss\n",
    "\n",
    "            accuracy += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "            total += len(labels)\n",
    "            test_loss.extend([loss.item()]*feats.size()[0])\n",
    "            del feats\n",
    "            del labels\n",
    "    return np.mean(test_loss), accuracy/total\n",
    "\n",
    "def get_auc(y_true, y_score):\n",
    "    auc = roc_auc_score(y_true, y_score)\n",
    "    return auc\n",
    "\n",
    "def test_verify(model, test_loader):\n",
    "    model.eval()\n",
    "    preds = torch.FloatTensor().cuda()\n",
    "    actuals = torch.FloatTensor().cuda()\n",
    "    with torch.no_grad():\n",
    "        for batch_num, (feats1, feats2, labels) in tqdm(enumerate(test_loader)):\n",
    "            feats1, feats2, labels = feats1.cuda(), feats2.cuda(), labels.cuda()\n",
    "            feature1, _ = model(feats1)\n",
    "            feature2, _ = model(feats2)\n",
    "            predicted = F.cosine_similarity(feature1, feature2)\n",
    "            preds = torch.cat((preds, predicted), dim=0)\n",
    "            actuals = torch.cat((actuals, labels.float()), dim=0)\n",
    "            del feats1\n",
    "            del feats2\n",
    "            del labels\n",
    "            del feature1\n",
    "            del feature2\n",
    "            del predicted\n",
    "    preds = preds.cpu().numpy()\n",
    "    actuals = actuals.cpu().numpy()\n",
    "    return get_auc(actuals, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af980548db7749a3a22d7e203df6b895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 100\tAvg-Loss: 1.5715\n",
      "Epoch: 1\tBatch: 200\tAvg-Loss: 1.5535\n",
      "Epoch: 1\tBatch: 300\tAvg-Loss: 1.5535\n",
      "Epoch: 1\tBatch: 400\tAvg-Loss: 1.5424\n",
      "Epoch: 1\tBatch: 500\tAvg-Loss: 1.5170\n",
      "Epoch: 1\tBatch: 600\tAvg-Loss: 1.4692\n",
      "Epoch: 1\tBatch: 700\tAvg-Loss: 1.4826\n",
      "Epoch: 1\tBatch: 800\tAvg-Loss: 1.4435\n",
      "Epoch: 1\tBatch: 900\tAvg-Loss: 1.4381\n",
      "Epoch: 1\tBatch: 1000\tAvg-Loss: 1.4179\n",
      "Epoch: 1\tBatch: 1100\tAvg-Loss: 1.4059\n",
      "Epoch: 1\tBatch: 1200\tAvg-Loss: 1.3954\n",
      "Epoch: 1\tBatch: 1300\tAvg-Loss: 1.3720\n",
      "Epoch: 1\tBatch: 1400\tAvg-Loss: 1.3806\n",
      "Epoch: 1\tBatch: 1500\tAvg-Loss: 1.3648\n",
      "Epoch: 1\tBatch: 1600\tAvg-Loss: 1.3664\n",
      "Epoch: 1\tBatch: 1700\tAvg-Loss: 1.3530\n",
      "Epoch: 1\tBatch: 1800\tAvg-Loss: 1.3478\n",
      "Epoch: 1\tBatch: 1900\tAvg-Loss: 1.3376\n",
      "Epoch: 1\tBatch: 2000\tAvg-Loss: 1.3416\n",
      "Epoch: 1\tBatch: 2100\tAvg-Loss: 1.3359\n",
      "Epoch: 1\tBatch: 2200\tAvg-Loss: 1.3223\n",
      "Epoch: 1\tBatch: 2300\tAvg-Loss: 1.3270\n",
      "Epoch: 1\tBatch: 2400\tAvg-Loss: 1.3184\n",
      "Epoch: 1\tBatch: 2500\tAvg-Loss: 1.2956\n",
      "Epoch: 1\tBatch: 2600\tAvg-Loss: 1.3200\n",
      "Epoch: 1\tBatch: 2700\tAvg-Loss: 1.3190\n",
      "Epoch: 1\tBatch: 2800\tAvg-Loss: 1.3151\n",
      "Epoch: 1\tBatch: 2900\tAvg-Loss: 1.2928\n",
      "Epoch: 1\tBatch: 3000\tAvg-Loss: 1.3188\n",
      "Epoch: 1\tBatch: 3100\tAvg-Loss: 1.3019\n",
      "Epoch: 1\tBatch: 3200\tAvg-Loss: 1.2913\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce6427b0aa046c084891a069a0c227e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Loss: 1.7999\tVal Accuracy: 0.6480\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f2afcaa17f4fcaad9f46cc3925d6af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Auc: 0.9217\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd92f01d9174603b3c0f7b65990b4b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\tBatch: 100\tAvg-Loss: 1.0686\n",
      "Epoch: 2\tBatch: 200\tAvg-Loss: 0.9968\n",
      "Epoch: 2\tBatch: 300\tAvg-Loss: 0.9630\n",
      "Epoch: 2\tBatch: 400\tAvg-Loss: 0.9542\n",
      "Epoch: 2\tBatch: 500\tAvg-Loss: 0.9309\n",
      "Epoch: 2\tBatch: 600\tAvg-Loss: 0.9109\n",
      "Epoch: 2\tBatch: 700\tAvg-Loss: 0.9101\n",
      "Epoch: 2\tBatch: 800\tAvg-Loss: 0.8830\n",
      "Epoch: 2\tBatch: 900\tAvg-Loss: 0.9186\n",
      "Epoch: 2\tBatch: 1000\tAvg-Loss: 0.8866\n",
      "Epoch: 2\tBatch: 1100\tAvg-Loss: 0.8879\n",
      "Epoch: 2\tBatch: 1200\tAvg-Loss: 0.8735\n",
      "Epoch: 2\tBatch: 1300\tAvg-Loss: 0.8767\n",
      "Epoch: 2\tBatch: 1400\tAvg-Loss: 0.8510\n",
      "Epoch: 2\tBatch: 1500\tAvg-Loss: 0.8823\n",
      "Epoch: 2\tBatch: 1600\tAvg-Loss: 0.8642\n",
      "Epoch: 2\tBatch: 1700\tAvg-Loss: 0.8525\n",
      "Epoch: 2\tBatch: 1800\tAvg-Loss: 0.8569\n",
      "Epoch: 2\tBatch: 1900\tAvg-Loss: 0.8540\n",
      "Epoch: 2\tBatch: 2000\tAvg-Loss: 0.8312\n",
      "Epoch: 2\tBatch: 2100\tAvg-Loss: 0.8424\n",
      "Epoch: 2\tBatch: 2200\tAvg-Loss: 0.8400\n",
      "Epoch: 2\tBatch: 2300\tAvg-Loss: 0.8211\n",
      "Epoch: 2\tBatch: 2400\tAvg-Loss: 0.8440\n",
      "Epoch: 2\tBatch: 2500\tAvg-Loss: 0.8504\n",
      "Epoch: 2\tBatch: 2600\tAvg-Loss: 0.8469\n",
      "Epoch: 2\tBatch: 2700\tAvg-Loss: 0.8238\n",
      "Epoch: 2\tBatch: 2800\tAvg-Loss: 0.8375\n",
      "Epoch: 2\tBatch: 2900\tAvg-Loss: 0.8462\n",
      "Epoch: 2\tBatch: 3000\tAvg-Loss: 0.8118\n",
      "Epoch: 2\tBatch: 3100\tAvg-Loss: 0.8229\n",
      "Epoch: 2\tBatch: 3200\tAvg-Loss: 0.8303\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6962bacbdf0492ca2e96788ba93a20b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Loss: 1.2256\tVal Accuracy: 0.7598\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa83a958f014adeb43b1e231c969e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Auc: 0.9342\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b1725a2e7841bcb16165d3764a9022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\tBatch: 100\tAvg-Loss: 0.7124\n",
      "Epoch: 3\tBatch: 200\tAvg-Loss: 0.6978\n",
      "Epoch: 3\tBatch: 300\tAvg-Loss: 0.6982\n",
      "Epoch: 3\tBatch: 400\tAvg-Loss: 0.6938\n",
      "Epoch: 3\tBatch: 500\tAvg-Loss: 0.6744\n",
      "Epoch: 3\tBatch: 600\tAvg-Loss: 0.6653\n",
      "Epoch: 3\tBatch: 700\tAvg-Loss: 0.6767\n",
      "Epoch: 3\tBatch: 800\tAvg-Loss: 0.6769\n",
      "Epoch: 3\tBatch: 900\tAvg-Loss: 0.6717\n",
      "Epoch: 3\tBatch: 1000\tAvg-Loss: 0.6793\n",
      "Epoch: 3\tBatch: 1100\tAvg-Loss: 0.6677\n",
      "Epoch: 3\tBatch: 1200\tAvg-Loss: 0.6784\n",
      "Epoch: 3\tBatch: 1300\tAvg-Loss: 0.6944\n",
      "Epoch: 3\tBatch: 1400\tAvg-Loss: 0.6868\n",
      "Epoch: 3\tBatch: 1500\tAvg-Loss: 0.6733\n",
      "Epoch: 3\tBatch: 1600\tAvg-Loss: 0.6688\n",
      "Epoch: 3\tBatch: 1700\tAvg-Loss: 0.6686\n",
      "Epoch: 3\tBatch: 1800\tAvg-Loss: 0.6853\n",
      "Epoch: 3\tBatch: 1900\tAvg-Loss: 0.6829\n",
      "Epoch: 3\tBatch: 2000\tAvg-Loss: 0.6841\n",
      "Epoch: 3\tBatch: 2100\tAvg-Loss: 0.6697\n",
      "Epoch: 3\tBatch: 2200\tAvg-Loss: 0.6740\n",
      "Epoch: 3\tBatch: 2300\tAvg-Loss: 0.6770\n",
      "Epoch: 3\tBatch: 2400\tAvg-Loss: 0.6617\n",
      "Epoch: 3\tBatch: 2500\tAvg-Loss: 0.6729\n",
      "Epoch: 3\tBatch: 2600\tAvg-Loss: 0.6833\n",
      "Epoch: 3\tBatch: 2700\tAvg-Loss: 0.6679\n",
      "Epoch: 3\tBatch: 2800\tAvg-Loss: 0.6605\n",
      "Epoch: 3\tBatch: 2900\tAvg-Loss: 0.6515\n",
      "Epoch: 3\tBatch: 3000\tAvg-Loss: 0.6779\n",
      "Epoch: 3\tBatch: 3100\tAvg-Loss: 0.6569\n",
      "Epoch: 3\tBatch: 3200\tAvg-Loss: 0.6730\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0dd31d8f464c8e926ee1df8262d4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Loss: 1.1114\tVal Accuracy: 0.7796\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e48c2af792e43feaa2d311b7747d780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Auc: 0.9371\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41e1279b4594e628cf3c50dba55f199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\tBatch: 100\tAvg-Loss: 0.6554\n",
      "Epoch: 4\tBatch: 200\tAvg-Loss: 0.6555\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-9e9c2e026a44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_closs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverification_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-62-273847bb29e9>\u001b[0m in \u001b[0;36mtrain_closs\u001b[0;34m(model, data_loader, classification_test_loader, verification_test_loader, task)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0ml_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion_closs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcloss_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mc_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-6e9840c6d122>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, labels)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mdistmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_closs(model, train_dataloader, dev_dataloader, verification_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), MODEL_PATH+'closs_model_%d.pt'%int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestImageFolder(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.file_list = []\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.jpg'):\n",
    "                    self.file_list.append(file)\n",
    "        self.file_list.sort()\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.file_list[idx])\n",
    "        to_pil = torchvision.transforms.Compose([torchvision.transforms.ToPILImage()])\n",
    "        image = to_pil(plt.imread(img_name))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TestImageFolder(DATA_PATH+'test_classification/medium', \n",
    "                                               transform=data_transforms)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                                             shuffle=False, num_workers=num_workers,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4600"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000.jpg\n",
      "torch.Size([32, 32])\n"
     ]
    }
   ],
   "source": [
    "print (test_dataset.file_list[0])\n",
    "print (test_dataset[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAH/NJREFUeJztnXusZXd13z/rnHPPfc/jznjGdx722HigMVVs6MhCpY1oQlKCVBnaNOIhglTUSZOQFjWValGR0CpSSBRAVEpJhhjhEIJDAhQS0RaEgixaajJQ2xgGY7DHrxnPjGfmzn2fxz2rf5xjen2717qvued6vL8f6ereu9f57b3Ob5919j6/71lrmbsjhCgfle12QAixPSj4hSgpCn4hSoqCX4iSouAXoqQo+IUoKQp+sWHM7AYzmzWz6nb7ItaPgv8axsy+ZmaLvQCcNbNHltneZmZPmNmcmf1XM5tYZpsws8/3bE+Y2dtW7Dccuxx3f9Ldx9x9aeuepdgqFPzXPu/uBeCYu78CwMxeCfwR8A5gPzAP/JdlY/4AaPZsbwc+2huzlrHiJYKC/6XJ24G/cvf73H0WeB/wT81s3MxGgX8GvM/dZ93968AX6QZ7OnblQczsiJm5mdV6/3/NzH7HzL5pZlfM7Asr7jh+qXdHcdHM3mdmp83s9Vs6EyJEwX/t8ztm9pyZ/U8ze11v2yuBB59/gLv/iO6V/uW9nyV3/8GyfTzYG7Pa2LXwS8C/AA4AbeA/A5jZrXTvIN4OTAI7gYNrfpbiqqPgv7b598DNdIPoBPBXZvYyYAy4suKxV4DxVWyswb4an3T3h919ju5dwy/2FgR/ge4dxdfdvQn8JqDEkm2ktt0OiI3j7vcv+/ceM3sr8EZgFtix4uE7gBmgk9hYZexaeGrZ308AA8BeuncCP7a5+7yZXVzjPsUWoCv/SwsHDPgucNvzG83sZmAQ+EHvp2ZmR5eNu603hlXGroXDy/6+AWgBzwFngUPL9jsM7FnjPsVW4O76uQZ/gF3APwaG6N7BvR2YA15B93P7NPAPgVHgT4F7l429F/h0z/Zaurf1r+zZ0rErfDhC9w2n1vv/a8DTwK3ACPAXwJ8t2+8M8PeBOvB7dN8YXr/dc1nWH135r10GgN8GLtC9sv468CZ3f8Tdvwv8K+BTwHm6n9d/ddnYXwWGe7ZPA7/SG8NqY83sv5nZexO/Pgl8AniW7hvTv16231+n+8Zzlu4bwXmgsdEJEJvDeu/KQqyb3keCR+le+d3Mvgb8qbv/8RrGjgFTwFF3f3xrPRVF6MovNsPfBU77Gq8gZvZPzGyk912D3we+A5zeQv9EgoJfbAgz+7d05cW71jHsTuBM7+co8Ja1vnGIq49u+4UoKbryC1FS+voln2q14gPV4kMODQ6G4wbr9cLttUr83pW9q9kGbR58Ia2S+NFeihPesnFmsa3TifdpwTOoVjaadRvfGWb3jNFcbcV9ZnbOQjqxJ9EcApht6Gh00nkstnWSu/LIMjUzy9zi4pqc3FTwm9kbgI8AVeCP3f0D2eMHqjWOXL+/0HbLjUfCcbfceEPh9r1DI+GYocSPwSSwqhZPeKfTLt7fyHA45tL0ym/K/j+GknFDg/Fzm5udDW11GyjcvnMs/nZuZyl5kVnxcwZodzqhrUWxbSkJgsxWTQKykthqga2z2AzH1JPyBNVaHDKehFwzmA+AhaVW4fZFj+e+GVwA/vDzfx07sYIN3/b3vq/9B8DP0/1Sx1t7yRtCiGuAzXzmvwP4obs/5t1EjXvpruYKIa4BNhP8B3lhEsfTFKRomtlxMztpZieXkttEIUR/2UzwF33C+f8+tLn7CXc/5u7HqskClxCiv2wmGp/mhRlch+h+eUMIcQ2wmdX+vwWOmtlNwDPAW4C3ZQM63mF+Ya7QdiVZFZ+eni7cvqNavLINMJJIh5VktZ+kFuVGZJ5qNV45zvaX2bJ9RupQq1W8ogzQbsaryotLi6GtUktWxevF56YeyLYAXomfc/ZlNGsnHyeXim2ZzOrZed6A/NZ1I35dtdvF89/qxOessVQ8JpMHV7Lh4Hf3tpm9G/gfdKW+jz+fGSaEePGzKZ3f3b8EfOkq+SKE6CNagROipCj4hSgpCn4hSoqCX4iS0tesPnen1SqWKKauTIXjLl9eWUm6y97RsfhgQ0lqTyIpkXSdi6SoTF6xavL+mviRZYHVBmKJ07z4eJmMlvkxsbOwTR+QJ+K0vVhia6eSV2zLpL5KkphUDUyV5LxYkqHTyV46wXMG6GRJUIFs12jFyUeNdrEM6Ov4Fq2u/EKUFAW/ECVFwS9ESVHwC1FSFPxClJS+rvabGbV68SEXmnHjlqkrxUk/sxPxSnR7ZDS0Zavlae2/IBkkLT81kJV9ipeON7rPaLV/KUhwWe1YU1OxCrOUTlagjFQ3pjpUk2SsLAkqmuOkWluaoZOt2neS1f6NKAHNIOGnawtW+9dRJVFXfiFKioJfiJKi4BeipCj4hSgpCn4hSoqCX4iS0lepD8ADuWyxEScxTE0Xy02XL18Ox0wkUt9A0r2mNhDXpasMFPueJWDUBuOadVnST1aKLerWArA4U9zN58pUXCOxmXSvOfvMk6GtPhwnT43uKJ7jsd07wzE7xuPzMpQkalVriXQbJOl0kjZqmda3UTkvSnSCWGrN9rcU+L+exru68gtRUhT8QpQUBb8QJUXBL0RJUfALUVIU/EKUlG2o4VcsK3kzll5mAvnquYsXwzF7dxTX/QPYOTIS2oZqmTRXLBstNWNJZjBoWwW5/DMzOx/a5udi24VnLxRuP3/mbDimMR+35NqTZE42W/E5W7gcyLPTxecSYGgsPi87kvO5Z0csH44NDhcfK5EHs9ZanaReYCuR2ZaycUG7sVY2v43irL5Opw/tugDM7DQwQ7fsZdvdj21mf0KI/nE1rvz/yN2fuwr7EUL0EX3mF6KkbDb4HfiymX3LzI4XPcDMjpvZSTM7uZ72wUKIrWWzt/2vdfczZrYP+IqZfd/d71v+AHc/AZwAGKhFLRSEEP1mU1d+dz/T+30e+Dxwx9VwSgix9Wz4ym9mo0DF3Wd6f/8c8J+yMU6cqRQk+wHQ9mLJY35hIRwztxjbsmyp7NYkaquUOZ+1d7ocFCYFeObMs6Ft5sp0aLt44VLh9iuXircD1Cqx7HXLnltC23wyx9NzxZLe9Hz8nG1+JrTNzs+FtsZ87MeOILvzyOTBcExW0DTKpgNoBa9TiAtuAjSaxfL3wmIswUZzv56P1pu57d8PfL5XObUG/Jm7//dN7E8I0Uc2HPzu/hhw21X0RQjRRyT1CVFSFPxClBQFvxAlRcEvREnpe6++gaBPXq0Wa2IDQZ+2zgYlmazIYWZrBXJNJemd10r6rV1KCpCefjIunDk3E0t9zcXi40V9BgF27t6V+PFEaPOst161uBBqfWgwHJP1UMz8n0lkwMVADr7xwKFwTFDzE8hlwFbymmsnmYKN4HW1mBSGXQx6W3oiY69EV34hSoqCX4iSouAXoqQo+IUoKQp+IUpK/1f7gxp5zSQ5g2ildHQsHFKrbeypZUk/0Qp2JTnWQpC0AfC/7v9GaNt73fWh7e/c+srQdu6Z4oSgG2+8MRwzn9QLHB2MV+ezFXiCVmSzC0ltwkacyBKpRADeihWVZ58tno9Lh2OlZed4XC8wS9DpdJL2WomCECXpNIIVfYiT2jIfVqIrvxAlRcEvRElR8AtRUhT8QpQUBb8QJUXBL0RJ6a/UR/xuU7UksadSLLHVgu0AVRJtJaGSjQukLUt8J0l+OTB5ILRdt38ytB0+ENef2zVenKSz/7rrwjHtZiyVZfOYJZ5ESS4Do0mrtKRmXUYjaV9WGxoq3H7hYlzTcDho8QVA8prLZLZGM5YIm4FUmXT4IjrSespj68ovRElR8AtRUhT8QpQUBb8QJUXBL0RJUfALUVL6KvVBLB1ZkrU1ZMXyykiScZZlgVWSbLTM5lYssCwkbcOmZuMWVIOBDLUazahtGDA8XCxTZTLUUOLHYC2ex9Z03HqrFWRHps+5Fsto3k5qMibPbWx8vHD7c5djqe/6fftCWzXxcSnLwksyFiPJtJXU/WsGUup6GmGveuU3s4+b2Xkze3jZtgkz+4qZPdr7vXvthxRCvBhYy23/J4A3rNh2F/BVdz8KfLX3vxDiGmLV4Hf3+4CV90h3Avf0/r4HeNNV9ksIscVs9DP/fnc/C+DuZ80s/JBkZseB4wC1oLqLEKL/bHk0uvsJdz/m7seyxTQhRH/ZaDSeM7NJgN7v81fPJSFEP9jobf8XgXcCH+j9/sJaBhlGNbj614PCngDjgXw1Hsg4AMODsaQ0UI2fdpYLaIF1ZCTOVLuYyGE7dsSFIrO2YfOzcXuqkcFiX2bn4jHjI0nmXpJpNzWdtA3zYinKktZm7aS1WTYflaCwKsDYWHGR1+lLcQFPkizNrGhpI5Fgm604q68dyHbRdoBGow/tuszs08A3gFeY2dNm9i66Qf+zZvYo8LO9/4UQ1xCrXvnd/a2B6Weusi9CiD6iFTghSoqCX4iSouAXoqQo+IUoKX0v4FkNMvSGh2Kpb0cg14wlvfpGEqmvaht7z4vkJksyqZYSuSbzsZVUb8xkr4Ggb2Ark5oSiS2VvZJvbFYCFzuJ74tJVlyalViNMw/rQ8WZn/XheO4riRzZWYr9aCdZeNk5i1hK9tcIMgGz+V2JrvxClBQFvxAlRcEvRElR8AtRUhT8QpQUBb8QJaW/Up8Zg0FhzeGkGOfIUHFW31A9HpP1/iORjUiknEClpJEU8Gw14352nrhYr8fSZy2Q8wCqQYZblE0JuQw4MhpnTo4ltmaQXTbXjLMEk6knS1ZrJ5cwaxXLZdVKLA+224mcl8miybXUkkzSpSBbtJ3KvYkba0RXfiFKioJfiJKi4BeipCj4hSgpCn4hSkp/V/srFq5iZ6vbA5VgmT1Ztc9qpjWTFfiBLFklaNWU1fAbTFSMy0l9v0oteW5JXT3axcvA4yOj4ZDpmbil2IXLU6EtqxYXzdVCO577qC4d5O3XsuSphaB24dJCfKyZ+bjeYSWr8rjBNnCR/1kNP6KYSKtQrtjFmh8phHhJoeAXoqQo+IUoKQp+IUqKgl+IkqLgF6Kk9LmGn4WtsmobqKuXJVm0GrGk1B6IZcXOYCI5BtOV1U1rLcaS0jNPPR3a2kmWy8JinEg0XC+WHffv2x+OmboYt66aTmTFWiK/1ceK/Ujr/iVtt3bt2hXalkham83PF25vzsdzOJe0NsvawEXyJqwi9QWv46xuYZTcZVlC20qfVnuAmX3czM6b2cPLtr3fzJ4xswd6P29c8xGFEC8K1nK5/QTwhoLtH3b323s/X7q6bgkhtppVg9/d7wMu9cEXIUQf2cyC37vN7KHex4Ld0YPM7LiZnTSzk63s64pCiL6y0eD/KPAy4HbgLPDB6IHufsLdj7n7sYFkQUcI0V82FPzufs7dl9y9A3wMuOPquiWE2Go2JPWZ2aS7n+39+2bg4ezxP8bBm4Esk9wULLSKpZBWZzYc06gkrZPG40w76kk7KYozBaud2PnDR24MbY8/9WRom74Uy29Tl+NswPauYqnnlZMHwzHPXYmz+uy5OKtvKZG2mgvFEuHYvolwzNjunaGtVo0lrGYipw4GEls9zIqDwaSG38SOWOqbXiiWFQEW5+LXan0okkxjuXoxateV5lq+kFWD38w+DbwO2GtmTwO/BbzOzG4HHDgN/PKajyiEeFGwavC7+1sLNt+9Bb4IIfqIvt4rRElR8AtRUhT8QpQUBb8QJaWvWX3uTrMdyGWtWMpZCuSLepLA1Kyvv2AigCVtkKJWWFFW1mpE+wPYtSOWvYYH44KhozuLx+3ZuyccM3nwQGizIEsQoJX00PLB4pfW4K64xddoIvVlWXGz07FUudgsPte1apyRmLVDy7LmhodiGXCoGdtYLM4ibCdFaOlsvl+XrvxClBQFvxAlRcEvRElR8AtRUhT8QpQUBb8QJaWvUl/HOyy2i7O9Oo2kWGEgA1aSwo2JMsTCQly8MSv8ORgUn9wxOhaOyYqMTuyK5bdKLZai0h5ugXyY9bobGY/9P7hzb2jzWjzJrVrxOVvw2Pe5ZlwsdGoqzi6cn5oObc254nN99FCcbTkyOBzaLHlhJYmHcb/JhGqi5tUDqdLUq08IsRoKfiFKioJfiJKi4BeipCj4hSgp/V3tBxaWile/28nKvQWr25Vk5bheiVc9F4L6cgDtIBEEoBPU95ubiuuztTrx/rJ33tHhOKGmkqzczzeL69ktBNshbzP11LPPhrYdu+MWWqO7dhQb2slqf9Ima3Y2nuNqsgI/MVFcM3AoScIZHI5X++uJCrO4EPuftd4aDPaZvQZoFZ+zbC5Woiu/ECVFwS9ESVHwC1FSFPxClBQFvxAlRcEvRElZS8eew8CfANfTVetOuPtHzGwC+HPgCN2uPb/o7nGPKbqJPXNLxYkz9aRfVyT1DSTySSaxZXX6akldvcFacZsvD+TL1Y5VD/YHuTTUSBKTzjx3vnD7uUtxl/X5ZH+HxuK6eoudpHZhq1haXAxqOEJeW3GjRPM4txg/57GRWGKzRBbN6gxmdQHHx4oTq5aSS3N7unjur3ZiTxv4DXf/CeA1wK+Z2a3AXcBX3f0o8NXe/0KIa4RVg9/dz7r7t3t/zwCngIPAncA9vYfdA7xpq5wUQlx91vWZ38yOAK8C7gf2P9+pt/d739V2Tgixdaz5671mNgZ8FniPu09n9ctXjDsOHIf8M5EQor+sKRrNbIBu4H/K3T/X23zOzCZ79kmgcKXJ3U+4+zF3P1ZJvm8vhOgvqwa/dS/xdwOn3P1Dy0xfBN7Z+/udwBeuvntCiK1iLbf9rwXeAXzHzB7obXsv8AHgM2b2LuBJ4J+vtqOOO/OtYqmvk7hS9WK9LLuPsKDeHkB1oB7aBgdi+W2oXmyrWpwFlkl2Y8OxMjqT1LObbcS26StXCrc3lmI/2oke+ZO3HA1tmZw6Nz9fuL2ZyKIenGeASiKZzszE7bouBzX8Ro/E53kmySDMfMzq+42Px23KKq3i12OjGdeTnLHiY63n3nrV4Hf3ryf7/Jl1HEsI8SJCK3BClBQFvxAlRcEvRElR8AtRUhT8QpSUvhbwdKBJUIwzVqLCjL9kCN6JRY9Mfmu3Y1uzWZyRtjgfZ4hVkyzBxmIs5czOx8Ugs2yv0R3FktJ4PS48GRbbBJ6+cC60ZbJXOCbTojqJjJZlRyYFTauDxedzZ1J89PLluDXY5aRt2K49xcVCAcaSlmjNQDIdSDIBd44U708FPIUQq6LgF6KkKPiFKCkKfiFKioJfiJKi4BeipPRV6uuKfRuQh7xYrul04veuZlIocj6V2OKMuVqQSbVjLM7YyqS+2mCcXdi4HPfWm1+Kn9tikA1YTYTRkUSym2/EMuZw0tMuyoBsNOLnNT0zHdoas7H0aYlEGPXW23f9/nDMuXOxvHnx4sXQVh2Kz+fw+GhoiwqXRq83gL27dhePqa49pHXlF6KkKPiFKCkKfiFKioJfiJKi4BeipPR1td/MGAgSDzxp1VQN6upNBCueAKNDcculZ89dCG3PPP5EaGsvFq9UHzx4MBxz7O+9KrQdPHQ4tD11objtFsDsVNx6a+f+vYXblxKV5fz5+Fj1SqxWjNZilePxJ04X78/i/ZHUGWw1YoVmOkm2uf324vnfvz9e7f/y+Xi1f2ExVoO+98j3Q9vjTz0Z2m644YbC7Z12HBOVQMVYD7ryC1FSFPxClBQFvxAlRcEvRElR8AtRUhT8QpSUVaU+MzsM/AlwPd2yeSfc/SNm9n7gXwLP62bvdfcvpfsC6sH7TdbEczhor1Wvx4kU9cG4HRMWt4xaasUSigfSyxNnzoRjnjoX23ZP7Alte/bFtn0HYmnx4pXixJO5pMXX/n3XhbZmKsEmLdaChKYLZ2IZ7cqlWMKc2BnXx7vpyE2h7VW33Va4/czZs+GY2YXiVmMASWlIKknNvaUgOQ1gfrE4eWphLvaj3SpO7uokLdRWshadvw38hrt/28zGgW+Z2Vd6tg+7+++v+WhCiBcNa+nVdxY42/t7xsxOAfGlRwhxTbCuz/xmdgR4FXB/b9O7zewhM/u4mcVftxNCvOhYc/Cb2RjwWeA97j4NfBR4GXA73TuDDwbjjpvZSTM76UnRBSFEf1lT8JvZAN3A/5S7fw7A3c+5+5J3y+x8DLijaKy7n3D3Y+5+zJJFPSFEf1k1+M3MgLuBU+7+oWXbJ5c97M3Aw1ffPSHEVrGW1f7XAu8AvmNmD/S2vRd4q5ndTrco32ngl1fbUcUh6J5EtRa/Dw1Vi+W3rE2TJdlobYvlkMVEklnsFEuEtSRTrRHIOABzz8WyV8NiP3bviVtNjY8XZ9rt2hMvyWS1+CyRU0myzqpB/bmBRB7MaiHeclMs573i5S8PbWPBfDz4v78RjpmanQltI0nbraHkuVGNX99RXcNWM85krFjxXfR6WqitZbX/63Ql+pWkmr4Q4sWNvuEnRElR8AtRUhT8QpQUBb8QJUXBL0RJ6WsBz4oZo7Vi6ahWi+WysXqxFDUS7AvyNlmNpJjlYjvJvoraKlXjMfWxuJDofJK19cMnH4/3+Uz83K6f3Fe4/cCBycLtABdnZkPb5fm4TdaVS5dDWydoibYzOJcAhyfjlJGjR26Oxx08FNrazeLst8dOx/M714zl2aFKLPVV6kk4ZdJzIC9nr+Eoo9UCCbDQpTU/UgjxkkLBL0RJUfALUVIU/EKUFAW/ECVFwS9ESemr1Fe1CrsDqaeaZOgNDwZS39BQOKYTZJUBeCK7LNViqaQTSC/zxNltU9NxH7l2krU1sXtnaMt69Z363o8Kt1cTCWjfdXEBz8Ug4wygkRQF3TNW7P+hvXGPvMldcdHSnUF2HsQZhAAXp68Ubr+U9PezRGJrE8u6jax4ZtCjEmAxKMY5lPgRyYBWmIMXuLTmRwohXlIo+IUoKQp+IUqKgl+IkqLgF6KkKPiFKCl9lfpqlSoTw8WSTSXJ6huoF0t6VYvdb3gsu7SSIp3NpADiYvBWWUuyC7PeaZmtOphliMXv2YNDxVLPDYcPh2MO7L8+tM0n0lY9kUwP7CyW7W6ajDPwbnvFraFtYndcgPRi0uPv1Pe/X7i9k/QgrA/FfR6byTmbb8ayaFpYMygoW0leV82oV986Cnjqyi9ESVHwC1FSFPxClBQFvxAlRcEvRElZdbXfzIaA+4DB3uP/0t1/y8xuAu4FJoBvA+9w9zhTBRgcGAhXe5csXqVsB0kMraTxZztJOmkl3YIbS8WrqACL7eKnF3TxAmBkKF6xnZuL20KdefZ8aGstxAe85dDewu2T++KEmqlLF0Pbk089GdqWFpLTPVe88l1Jxtx29CdC297dE6Ht0sV4tf/Jx4pr9XWSy16tHieZNZaSk5285qqJQmOBojJYicOzFaz2r6dd11qu/A3gp939NrrtuN9gZq8Bfhf4sLsfBS4D71rzUYUQ286qwe9dni/vOtD7ceCngb/sbb8HeNOWeCiE2BLW9JnfzKq9Dr3nga8APwKm3P35e6CngbjushDiRceagt/dl9z9duAQcAdQ9OGs8MOGmR03s5NmdnIh+JwihOg/61rtd/cp4GvAa4BdZj/+fu0h4Eww5oS7H3P3Y8NJtR4hRH9ZNfjN7Doz29X7exh4PXAK+BvgF3oPeyfwha1yUghx9VlLYs8kcI+ZVem+WXzG3f/azL4H3Gtmvw38H+Du1XY0ODjILTfdVGjLJJSoTdZ0kkjRnCmu3QZQnZkObSTy4VLwVrmUJIk0Els7SRIZHYnbfI0OxHLZ6GhxO6m5ubjt1qOPPBLaarX4JbJ7T1xXr9IoPp+nvncqHHNoorjVGEAn+ch4PpEqLwa2zkB8nivJa6DZSOr0VRNbUkOxEtQgtEweDPa3jm5dqwe/uz8EvKpg+2N0P/8LIa5B9A0/IUqKgl+IkqLgF6KkKPiFKCkKfiFKiq0nC2jTBzO7ADzR+3cv8FzfDh4jP16I/Hgh15ofN7p73H9tGX0N/hcc2Oykux/bloPLD/khP3TbL0RZUfALUVK2M/hPbOOxlyM/Xoj8eCEvWT+27TO/EGJ70W2/ECVFwS9ESdmW4DezN5jZI2b2QzO7azt86Plx2sy+Y2YPmNnJPh7342Z23sweXrZtwsy+YmaP9n7Hzem21o/3m9kzvTl5wMze2Ac/DpvZ35jZKTP7rpn9m972vs5J4kdf58TMhszsm2b2YM+P/9jbfpOZ3d+bjz83s7g09Fpw977+AFW6NQBvBurAg8Ct/faj58tpYO82HPengFcDDy/b9nvAXb2/7wJ+d5v8eD/w7/o8H5PAq3t/jwM/AG7t95wkfvR1TgADxnp/DwD3062e9RngLb3tfwj8ymaOsx1X/juAH7r7Y96t838vcOc2+LFtuPt9wMpi83fSrYIMfaqGHPjRd9z9rLt/u/f3DN1KUQfp85wkfvQV77LlFbO3I/gPAk8t+387K/868GUz+5aZHd8mH55nv7ufhe6LEIjL2mw97zazh3ofC7b848dyzOwI3eIx97ONc7LCD+jznPSjYvZ2BH9RoaHt0htf6+6vBn4e+DUz+6lt8uPFxEeBl9Ft0HIW+GC/DmxmY8Bngfe4e1Jrre9+9H1OfBMVs9fKdgT/08DhZf+HlX+3Gnc/0/t9Hvg821uW7JyZTQL0fsf9urYQdz/Xe+F1gI/RpzkxswG6Afcpd/9cb3Pf56TIj+2ak96x110xe61sR/D/LXC0t3JZB94CfLHfTpjZqJmNP/838HPAw/moLeWLdKsgwzZWQ34+2Hq8mT7MiXWrUd4NnHL3Dy0z9XVOIj/6PSd9q5jdrxXMFauZb6S7kvoj4D9skw8301UaHgS+208/gE/TvX1s0b0TehewB/gq8Gjv98Q2+fFJ4DvAQ3SDb7IPfvwDurewDwEP9H7e2O85Sfzo65wAP0m3IvZDdN9ofnPZa/abwA+BvwAGN3Mcfb1XiJKib/gJUVIU/EKUFAW/ECVFwS9ESVHwC1FSFPxClBQFvxAl5f8CArvCxzY11ZYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs = test_dataset[0]\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "imshow(out,title=test_dataset.file_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f36681203b244f686fd190e545939e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5000.jpg</td>\n",
       "      <td>1631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5001.jpg</td>\n",
       "      <td>908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5002.jpg</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5003.jpg</td>\n",
       "      <td>1359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5004.jpg</td>\n",
       "      <td>1782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id Category\n",
       "0  5000.jpg     1631\n",
       "1  5001.jpg      908\n",
       "2  5002.jpg      189\n",
       "3  5003.jpg     1359\n",
       "4  5004.jpg     1782"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "image_names = test_dataset.file_list\n",
    "test_preds = torch.LongTensor().cuda()\n",
    "with torch.no_grad():\n",
    "    for batch_num, feats in tqdm(enumerate(test_dataloader)):\n",
    "        feats = feats.cuda()\n",
    "        outputs = model(feats)[1]\n",
    "\n",
    "        _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        test_preds = torch.cat((test_preds, pred_labels), dim=0)\n",
    "\n",
    "        del feats\n",
    "        del pred_labels\n",
    "    \n",
    "out_df = pd.DataFrame()\n",
    "out_df['Id'] = image_names\n",
    "out_df['Category'] = test_preds.cpu().numpy()\n",
    "out_df['Category'] = out_df['Category'].map(idx_to_class)\n",
    "out_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = SUBMISSION_PATH+\"submission_%d.csv\"%int(time.time())\n",
    "out_df.to_csv(file_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_test_dataset = VerificationDataset(DATA_PATH+\"test_trials_verification_student.txt\",DATA_PATH+\"test_verification\",data_transforms)\n",
    "verification_test_dataloader = DataLoader(verification_test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers,pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "899965"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(verification_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91674d8eb59d42d8875ba172383d7938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>262615.jpg 207587.jpg</td>\n",
       "      <td>0.274809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120800.jpg 162540.jpg</td>\n",
       "      <td>0.298063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200386.jpg 117646.jpg</td>\n",
       "      <td>0.317300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>268346.jpg 264478.jpg</td>\n",
       "      <td>0.246046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>171295.jpg 143107.jpg</td>\n",
       "      <td>0.288800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   trial     score\n",
       "0  262615.jpg 207587.jpg  0.274809\n",
       "1  120800.jpg 162540.jpg  0.298063\n",
       "2  200386.jpg 117646.jpg  0.317300\n",
       "3  268346.jpg 264478.jpg  0.246046\n",
       "4  171295.jpg 143107.jpg  0.288800"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "preds = torch.FloatTensor().cuda()\n",
    "with torch.no_grad():\n",
    "    for batch_num, (feats1, feats2) in tqdm(enumerate(verification_test_dataloader)):\n",
    "        feats1, feats2 = feats1.cuda(), feats2.cuda()\n",
    "        feature1, _ = model(feats1)\n",
    "        feature2, _ = model(feats2)\n",
    "        predicted = F.cosine_similarity(feature1, feature2)\n",
    "        preds = torch.cat((preds, predicted), dim=0)\n",
    "        del feats1\n",
    "        del feats2\n",
    "        del feature1\n",
    "        del feature2\n",
    "        del predicted\n",
    "\n",
    "preds = preds.cpu().numpy()\n",
    "out_df = pd.DataFrame()\n",
    "trial_id = pd.read_csv(DATA_PATH+\"test_trials_verification_student.txt\", header = None)[0]\n",
    "\n",
    "out_df['trial'] = trial_id\n",
    "out_df['score'] = preds\n",
    "out_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = SUBMISSION_PATH+\"submission_verification_%d.csv\"%int(time.time())\n",
    "out_df.to_csv(file_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
