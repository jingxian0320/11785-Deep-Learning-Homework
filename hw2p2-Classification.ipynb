{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data/11-785hw2p2-s20/\"\n",
    "MODEL_PATH = \"model/\"\n",
    "SUBMISSION_PATH = \"submission/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision   \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = torchvision.transforms.Compose([\n",
    "        #transforms.Resize(32),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder(root=DATA_PATH+'train_data/medium', \n",
    "                                                       transform=data_transforms)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,pin_memory=True)\n",
    "dev_dataset = torchvision.datasets.ImageFolder(root=DATA_PATH+'validation_classification/medium', \n",
    "                                               transform=data_transforms)\n",
    "dev_dataloader = torch.utils.data.DataLoader(dev_dataset, batch_size=batch_size, \n",
    "                                             shuffle=False, num_workers=num_workers,pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(822155, 2300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4600, 2300)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_dataset), len(dev_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_class = {v: k for k, v in train_dataset.class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAH61JREFUeJztnX1sned53q/7fPLwQyIpURQlUZ+W7Nhu7KSKF8xD0bVr4OYfJ0BXJBuCDMjmomuABeg2GBmwZsMGpNuSIH8MGZQlrVtkcbM4QYwlbWLYGdykrWPakWVbsvVl6lukJFL8Js/XvT/O8UDTz/WQFslD2c/1AwiSz32e9735nPfme85znfu+zd0hhEiPzEY7IITYGBT8QiSKgl+IRFHwC5EoCn4hEkXBL0SiKPiFSBQFvwhiZr1m9n0zmzGzc2b2TzbaJ7G25DbaAXHb8t8BlAH0A7gfwA/N7CV3f3Vj3RJrhekTfmIpZtYBYBzAve5+sjn25wAuufujG+qcWDP0sl+EOASg9mbgN3kJwD0b5I9YBxT8IkQngIklYxMAujbAF7FOKPhFiGkAm5aMbQIwtQG+iHVCwS9CnASQM7ODi8buA6DNvvcQ2vATQczscQAO4J+jsdv/IwB/X7v97x105xeMfwmgBGAUwLcB/L4C/72F7vxCJIru/EIkioJfiERR8AuRKAp+IRKlpYk9vZ2dPtjbG7RFNx7NwnMQHgeAOmLH4//znJwLAOrMx8icWr3Oz/XO/+S4HwBq1Vp4TsQPi5ys0JbnftT4MfM5cml5xA9qATx6rmxkXng9YmeLXYtRG7XEoesfuU4Zl8du4Ob0dGwp/z+rCn4zewjAVwFkAfxPd/9i7PGDvb348b/5t0FbpVblE3PhC7DsfHFm6vx4XmijtnquQG1TlXJw3PKRObNz1FaJBGSWBQ+AhYUFahsfHw+Oz81xP3KRcw0eHKS2+alJatve1xcc94jvxUhgVab5hwv7e3qorTwVnpeJBH+1zP5hAOVqhc+rR/4xRAI5R6652HVlmfBz9k//63+mc5Zyyy/7zSyLRtrnbwO4G8AnzezuWz2eEKK1rOY9/wMATrv7WXcvA3gcwMNr45YQYr1ZTfDvBHBh0e8Xm2NvwcweMbMhMxu6MT29itMJIdaS1QR/6E3T2970uPsRdz/s7oe3dHau4nRCiLVkNcF/EcDi3aBdAC6vzh0hRKtYzW7/8wAOmtk+AJcAfAJAtMijA6haeEfUsxH5LRe2zS3wndd8qURtC5H/eZkCX5Is8f3K9Wt0zvGTp6jt2MuvUNu5C+epbWx8jNrmyuFd/UKG7xx3dkVekeW5IrFr+w5qe/DvPRAcf/9dd9I5u/v7qa09pn4QFQYAZubmw8cr8vVAhisBMWUkpiDUo0Jm2FajMiVQr4bVrHeSq3PLwe/uVTP7LIAfoyH1fVNZX0K8e1iVzu/uP0Ijz1sI8S5DH+8VIlEU/EIkioJfiERR8AuRKC3N6nM45kjCTb6tyCdmw1lbU5M8sWRb3xZqK5PMNwAYuclltBNnzgTHj/zJn9A541Mz1DYP7kcOPFOtGJGpcm1hibNa5+eampultokJnhA0NxuW0QDg2rWrwfETrx6gc+45wG333XWI2nb2baO2PQf2B8dvjIzSObmoLMepeyRZKJKVyJKFakTOaxwv/Hx6JGtyKbrzC5EoCn4hEkXBL0SiKPiFSBQFvxCJ0tLd/jqAObLDbZE6bCCJPQvGdzZrOb7zOnojXOoKAH72/PPU9sOnngqOT1Z4aSrP87+rLcuXP1/k6kcuy+vqVarhJJeFOZ78Uo34X2rnfhS6Oqjt2vjN4Hg+e47OMfDnc1NHO7XNznBFhSW6ZCO74rEknGyG3y/rzhWV2q1U+COJZACQy4Z9fCc6he78QiSKgl+IRFHwC5EoCn4hEkXBL0SiKPiFSJSWSn3IGDJEOpqKyE35fFjmKfV20zkXx3hdvWf+5q+p7alnue10JBmEkY1IjgYu2ZXLfD3MuI0pUblSJBkoYpua5Ek/pXZe+69GkrGqEalsLlKzbixS9j3WjahC6vvdtf8OOieul0Uku2ibL/63sfMVYvI3qTNokfqDbzvEih8phHhPoeAXIlEU/EIkioJfiERR8AuRKAp+IRKlpVKfZTIodIRrzN28zmvnsdZKpU1cavrrnz5DbT955mlqOzsazkYDgI628HLVIv9C2zu7qK1QbKM2lp0HAJUKb1Pm9bDcVK/xenCx422KyKnVSBbeHJHYzl+9wo8XabvVvWkztR3YNcCPyaSvWFu2SLZlwSJSWqxV1gL/28rlsC3WrstrpO1dK9p1AYCZDQOYAlADUHX3w6s5nhCidazFnf8fuvv1NTiOEKKF6D2/EImy2uB3AD8xsxfM7JHQA8zsETMbMrOh8ampVZ5OCLFWrPZl/4PuftnMtgF4ysxec/dnFz/A3Y8AOAIA9+7fewu1jIQQ68Gq7vzufrn5fRTA9wE8sBZOCSHWn1u+85tZB4CMu081f/4IgP8Ym+MAKkSiyBUiRSlJq6npm1yW+/nPf05t46S4JAD0dvIMt+27B4PjN6d4xlm+jReejMoykQKTxRx/2jZ3h6W5zlJYYgXihSfPXuKZjAuRzMO2zrAMOzfJ3/pNzvPsvPNXLlNbezuXTKdYNmBEsusmvgPAls091NZV4n5ks/w+a8TmkRZr9Uj7r5Wympf9/QC+b41FzAH4X+7+V6v2SAjREm45+N39LID71tAXIUQLkdQnRKIo+IVIFAW/EImi4BciUVqa1ee1OhZmwgUhuzq4vDJLMqKunL9I57x+4hS1be3lcs3mvm3UxvLiDu3dT+cUIxKbR4otskwvAMhGKkwWiuGntBLJKpucmODnikhUsayzDvJ3T0ekvqlIIc6LkWzArk08c7JGjjk8PEzn7Ozvp7Y79/Pneu+usBQMAD0RH3OkUGc+E5GCSQ9Ii2UdLkF3fiESRcEvRKIo+IVIFAW/EImi4BciUVq6258xQ6kQbtdVK/Mac0UL72weO3qUzrkjsvNaz/Pknd179lFbe1d4x7avnysEhSLf7S+1cVu1xnfnZ2dmqK1GavVNRJKZzld5ksiltklqy+f4Dna9GlYCtvZtoXPmZ3mC1FhEJfjlsZeprburIzi+ezuv+/fGpUvUdurMaWq7cx9XAj58+EPUtmtn2JeOEk8KK7MkKO32CyGWQ8EvRKIo+IVIFAW/EImi4BciURT8QiRKS6U+AMgSValW4XJTLh92c6B3K51zubuX2u68+x5q27SVH7O9a1NwvBCpIRdLtMhmwxImAFQqXI4sOT9mhSQEZUq83l61myc6jUXq+81FJMfJyXCy0OQC9yNr/F7U3sPbhu3ewWW7HUTSy9T59TZ9c5zacpFkrOOneDJZrOLeRzb/ZnDcSMIPAGRZHceVK3268wuRKgp+IRJFwS9Eoij4hUgUBb8QiaLgFyJRWp7VV8iGT1ktc0kpnwm38tqzYxedc+HyCLXdsZdn7nmGL0mmGPbDwCWZTES+ymYi0lYb96MjMm9qMpyFV81wH2sd4cw3ALh3y2ZqmyH1GAHg+tiN4PjICH9ebo7zWoL5Ni6n9m7po7Y+IvXlIpLYyAhvUVaMyLNXIpmT5ZMnqW3/wTuC4wNlLmHu2bUzbIhcb0tZ9pFm9k0zGzWzVxaN9ZrZU2Z2qvmdC8VCiNuSlfyb+FMADy0ZexTA0+5+EMDTzd+FEO8ilg1+d38WwNiS4YcBPNb8+TEAH1tjv4QQ68ytbvj1u/sVAGh+p6VszOwRMxsys6EbU7waixCitaz7br+7H3H3w+5+eAspgyWEaD23GvwjZjYAAM3vfHtUCHFbcqtS35MAPg3gi83vP1jRLAdAFL1MJPuNZcZtH9hB5+zecZXaZqZ5NlrnpnDmHgDMz4aLJrZFZCgrhOVBALBIqlcxokUVcjzjb8HDLZ6ykc5Pm9t5IdFCFy8iad08026AtETriazvpcu8JVe5wqXgbGSNawj/4Tu282tntvILapuvVqht2y4uPVdZwU0Azx09Fhzft4dLhwXSDq1S5YVwl7ISqe/bAP4WwJ1mdtHMPoNG0P+WmZ0C8FvN34UQ7yKWvfO7+yeJKZyELIR4V6CP9wqRKAp+IRJFwS9Eoij4hUiUlmb11d1RJlJJPVJQsUyKUnZEstH27NlLbaeHh6lt28B2astWwjJKISb1xTLwJngfvOsz3Fat8CKY0yQzrlKep3M2ReS3XI1LWzGJs9AZfm7KCzxLsDLP/675iIS1KSI5dnSG/7Y58lwCQC2SATkzw/sJ7jh0iNomr1+ntldPhgt/lutc3iy2h9d3dp4/z0vRnV+IRFHwC5EoCn4hEkXBL0SiKPiFSBQFvxCJ0lKpzwwwUgCx7lzqq9fCkkeVjAPApl4u/9SHeYrbxBSX2GqkR975ixfpnHyeZ5xdvHyB2i4Mn6O2hXleOLNeDstlpSKX5XaxYpAA7v7AvdRWqfN1XJgNZ05WIv39ciQDD4j7X8xFsvrINfLLYy/TOeOTXM6bj0h941PcViPZlgAQFrKBuUgm43lSCLVc4dLsUnTnFyJRFPxCJIqCX4hEUfALkSgKfiESpaW7/Q6DZ8I75vXITm+uEK5ZNzPNd1czkbZKu3bvprajx1+htjrZ7e9o53XudkfOtXfvXmrrKvHd7Wi9Q5IMUozUudu1g+/2F9r5JTIxwdtrLZCd78ocVyqy0VZT/G++HkmamTh3PjgeS+6KtetqayvyeaPcj852/nx29YTrHU5E2qGVyLVfiyTILUV3fiESRcEvRKIo+IVIFAW/EImi4BciURT8QiRKa6U+r2OeJJ4skDp9AFDqCtdhq5PkEQDIF3lLq0Pvu4vafvCXP6S2ajUso8SSgYwrjhjcNchte/dQ2+YuXrsQJBmkvMDr423Z0kttN2/wtmdzs1yKmp8LPzeZiGTXQVpQAcBMjdfcu3aJS3Mn33gjOF4oddI5s2V+rq5O3mx2ocqv4U7jf1s2H75Wz50bpnPaSIs1lsgUYiXtur5pZqNm9sqisS+Y2SUzO9r8+uiKzyiEuC1Yycv+PwXwUGD8K+5+f/PrR2vrlhBivVk2+N39WQBjLfBFCNFCVrPh91kzO9Z8WxD+fCIAM3vEzIbMbGg88nFcIURrudXg/xqAAwDuB3AFwJfYA939iLsfdvfDPZ18k0UI0VpuKfjdfcTda+5eB/B1AA+srVtCiPXmlqQ+Mxtw9yvNXz8OgKfCLSJrhp58WPvyApdCqnNhKa3LuKxRXZiiNjMu5fyz332Y2o6feD04PhZpu7W1g2dz7ejmbbJi9Qmrs9z/UpFknTk/3swN/nZsZJ6fa6rC7x0L+fDzmcnx57lu/HIsT/Hnc3aO162bmyPyWy4iy7VxHyt1vh43bvIsx6nZOWorEVk608FfKY+QOoOV2sqz+pYNfjP7NoBfB7DVzC4C+CMAv25m9wNwAMMAfm/FZxRC3BYsG/zu/snA8DfWwRchRAvRx3uFSBQFvxCJouAXIlEU/EIkSkuz+ur1OmZIAcdMlv8fyuTD8lWBFPZsnIxLW4VIC60t3fTDiti7Z29wvC8i4xSZ9LYMs5G2VgvzXNqqkwKOlRqf05aNtMKKtMm6Xr1JbdVqWBIrRbLiPMOfl9Gb/FzzkYzQbCaSVsn8iMiiFX4q2lYOAIzXp0Ubyeor5Pj1XWoj8iApkBt87IofKYR4T6HgFyJRFPxCJIqCX4hEUfALkSgKfiESpaVSHwCQdneoO9dCsqQ3XT4i2VXLPLupVuGZWcUsl+b6+/qC46+fOkPnxHyM9Zg7fYYfs1TiBTzvuffe4HimwmWjK1d5kc5qiUtllUhRzRzpadfRwX0fn+YFQa9fu0ZtU5GMv7aOcB9Fy/D7Xi7LwyKT5VJaJdInz8BtRdJXslDk104HkWAz0X6HSx674kcKId5TKPiFSBQFvxCJouAXIlEU/EIkSmt3+w2wXHhns7zAd47dwtkU2Tx3v16J7PY7b12VjySylEiixV13HKRzfv53f0dtE9N8l3rbwA5q6+nl7bUujFwJjk+Qmm8A0Ne/jdqujY1Qm0XWv7MQXse5Ck8wGr50gdrOnB+mttl5/nx2k1Zk8ws8Q6dIkmaAeKJWNZL1U8jynfs2cr5cZOc+Uw+rYytP69GdX4hkUfALkSgKfiESRcEvRKIo+IVIFAW/EImyko49gwD+DMB2AHUAR9z9q2bWC+AvAOxFo2vP77r7+DJHQ51k9syXuVxTJQkkxTqXXTKRmmmxum75yP/DKpk3vcATUg4d4DLgxCyX+t64eInaRqd4PbuereHko4XIgpy8eI7aent4TcN8RFeaIEk6Z8+fp3NOvBZuhwYANyd5S7RCKZy8AwDZXPgSr0Q6Rmci10epjUvBdZKgAwCFiGxXzIR9rEfqLtbn54PjHkkuWspK7vxVAH/o7u8D8GEAf2BmdwN4FMDT7n4QwNPN34UQ7xKWDX53v+LuLzZ/ngJwAsBOAA8DeKz5sMcAfGy9nBRCrD3v6D2/me0F8AEAzwHof7NTb/M7/5iYEOK2Y8XBb2adAJ4A8Dl352/A3j7vETMbMrOhscj7LCFEa1lR8JtZHo3A/5a7f685PGJmA037AIDR0Fx3P+Luh939cG8n7zcuhGgtywa/mRkaLblPuPuXF5meBPDp5s+fBvCDtXdPCLFerCSr70EAnwLwspkdbY59HsAXAXzHzD4D4DyAf7zcgSxjKJTCUkm+ymUNYxlMxqUVNy55ZMHnsXZXAFBeCPvY3laicyYjbbcykUyvPKmBBwA3I9mAczfCte5KXfxV1779h6htdoKfazIiv71xMSzpHX/tOJ0zeoNLmKwWHwC0d/C/jbUNK0dafMU6Xlnk+siS6xQAchFdtIu0ncuAZxe2kzjKRmoTvs2n5R7g7j8DzxT8zRWfSQhxW6FP+AmRKAp+IRJFwS9Eoij4hUgUBb8QidLidl1G2yQVI9lS5uE5MWmlUq1xN7jSB9S4lFMhRR8jZ0J7O5eoRiYnqO34SZ7hNuvcx1/50AfDfmzeROeMTnA/Rs4OU9v4BJfmzp8PF+OciLTWKkTkze5IdmGskOvsXDj7LRvJwIsVwfR6pNBsRK4uEDkPAHrJc9O7aTOd09e7JTj+N6fP0jlL0Z1fiERR8AuRKAp+IRJFwS9Eoij4hUgUBb8QidJSqa9Wr2FyOpzlZpGCm8V8WCapgU/ySMHEHCnq2LBxSaZMpMVMnmfnDT3He/X97S9foLaTF3hRzdwmnsU2O/RicHymxrPYahHpsDbG5bxaja//LMmaK5FsNAAotHdQWzEyL5aJyYpxtrfzTEyP9NzLRoTAuvP1KGb59di3OSxj7tu1i84Z3Bm2lZ55hs5Ziu78QiSKgl+IRFHwC5EoCn4hEkXBL0SitHS332DIkl34Wp2nx7C6ZPk83wGeq/EWWvOkFh8AZCLJQplseLnKEd9ZIhMA3Pv++6jt5WGeoFFZmKO2k2dPBcdnIjvY1Yj/nQvcdt994SQiALhyfSRsIGsIANXITnohMm+2Gk7eAYCtW8I76VcvX6ZzKpVIgk4pohJEiv8NDuygtv7e3uB432ae2FMgStc7uZvrzi9Eoij4hUgUBb8QiaLgFyJRFPxCJIqCX4hEWVbqM7NBAH8GYDuAOoAj7v5VM/sCgH8B4M3+UJ939x8tcyzkSRKMx2rnVcO2unNJBjleo63QxpN3YtLc1FxYYitHkpLaN3dRW1skIWhwz25qmywvcBuR2OqRmoaFSDJTWy6SvDPJ6/FVSb3D/p28k3sssWd6hnd4vnad2xbIc8ZawAFANpIwlou0iEOkLmBPpKXYzm3hNdnet5XOyRPJfE3bdQGoAvhDd3/RzLoAvGBmTzVtX3H3/7biswkhbhtW0qvvCoArzZ+nzOwEgJ3r7ZgQYn15R+/5zWwvgA8AeK459FkzO2Zm3zQzXltZCHHbseLgN7NOAE8A+Jy7TwL4GoADAO5H45XBl8i8R8xsyMyGxqb4ezMhRGtZUfCbWR6NwP+Wu38PANx9xN1r7l4H8HUAD4TmuvsRdz/s7od7Iz3ihRCtZdngNzMD8A0AJ9z9y4vGBxY97OMAXll794QQ68VKdvsfBPApAC+b2dHm2OcBfNLM7gfgAIYB/N5yB3J31MtheS5n3BUmD8aoRurLxWr/LUSy38ZmJoPj+RJvyVUjNeQAoOKRTMYs/5vn5nl7rfmZsLSVL3I5b2tfuPUTAHxo8CC1ve+eu6ltbDK8VnOR9Z0hdf8A4FpERrt69Rq1XTx3MTi+ayfPssuSmpEA0BlZx2ykNuS2nm5q6+sOZ+91FnjWanmeZK1G6gguZSW7/T9DuH1ZVNMXQtze6BN+QiSKgl+IRFHwC5EoCn4hEkXBL0SitLiAJ2Ae/n8TbaFFMpjmazyrb7bCM99ymYgMGMkG9EJY5rkxzaW386NXqO3qdS5RnT03TG3zEbmMZegNDvDWT3sP7Ke2O/r4vPaIHFkvhTP0ujr5B72GL/Gimm+cOk1tN8fHqG0LKeDZ3lakcywiK/Zt7aO27jZe3HOgfzu1seKklTlehLYyH76+PZKtuBTd+YVIFAW/EImi4BciURT8QiSKgl+IRFHwC5EoLZX6MpZBZzEsh8xHilLO18K92ObrvOhnnbdNi8p5ZfBjztbD0uLR10/QOUdfO05tlyLS1nikOOa2SGHHfiIp7d+3j87ZuYNnuPXmuHzV28OzATuIFHWZ9fADUJ7lPQgLOS4r7urn/huRdSsL/Hpri8jO2/t5AdI9fdw2OMClvhzJxKtXuZTdTjJdMxa58Jc+dsWPFEK8p1DwC5EoCn4hEkXBL0SiKPiFSBQFvxCJ0lKpz91Rq1SDtgrp7QYAng9Lc6USl6FqNX68kbHr1Hbmwjlqe+2NN4Lj5yOS3eXRq9Q2Ox+WMAFgYIDLRgcPHKK2wR3hZkpbunlPlTzJKgOAHFc+MXXjJrcthGW7m+N8TnmWr0d9gcteHd38OmDS12jEj809fK36enqpbesWLn3GrtX6TDh7zyIJesVi+DkjCbBBdOcXIlEU/EIkioJfiERR8AuRKAp+IRJl2d1+M2sD8CyAYvPx33X3PzKzfQAeB9AL4EUAn3J3vsWOxm4/S6jwGt9WLrSH22GVnc8ZPn+e2l44wdsKvn4+vKMPABevh1WCjk3hdksA0B9JminmeTum2M5xfy+vI7e1K+wLqxMHAOV5vpO+qWcTtZ0+fYba6tnwLvvg9oHgOADki3w9zpw7S21M4QCAw7/6q8Hx7z7xBJ2zd88eatuzeze1tQUbWzWoRmpKZsh1HNvtnycKQSyO3nbeFTxmAcBvuPt9aLTjfsjMPgzgjwF8xd0PAhgH8JkVn1UIseEsG/zeYLr5a7755QB+A8B3m+OPAfjYungohFgXVvSe38yyzQ69owCeAnAGwE13f/MTOxcB8NdeQojbjhUFv7vX3P1+ALsAPADgfaGHheaa2SNmNmRmQ+PT06GHCCE2gHe02+/uNwH8XwAfBtBtZm/uIu0CEPyMq7sfcffD7n64J9KwQQjRWpYNfjPrM7Pu5s8lAP8IwAkAPwXwO82HfRrAD9bLSSHE2rOSxJ4BAI+ZWRaNfxbfcff/Y2bHATxuZv8JwC8BfGO5AxmMJpHEWiSBSChXr/JWWL8Yep7bXn2J2uqRNk4DO8OyXVck2WNggEtbXSX+Sqg9IntlK1wDymfC69hm4TZeAECmAADOvH6S2u6+51eoLdsWrjH35I//ks45fuoUtRXb+XrEWmGdOh1u83XojoN0zr138sSp/ft5a7Pxy5eorV6tUVueXPtZUjMSAKbIW+hapK7lUpYNfnc/BuADgfGzaLz/F0K8C9En/IRIFAW/EImi4BciURT8QiSKgl+IRDEnrYLW5WRm1wC8WSRvKwBeTK91yI+3Ij/eyrvNjz3uztM+F9HS4H/Lic2G3P3whpxcfsgP+aGX/UKkioJfiETZyOA/soHnXoz8eCvy4628Z/3YsPf8QoiNRS/7hUgUBb8QibIhwW9mD5nZ62Z22swe3Qgfmn4Mm9nLZnbUzIZaeN5vmtmomb2yaKzXzJ4ys1PN77xh3Pr68QUzu9Rck6Nm9tEW+DFoZj81sxNm9qqZ/avmeEvXJOJHS9fEzNrM7Bdm9lLTj//QHN9nZs811+MvzCJ52ivB3Vv6BSCLRg3A/QAKAF4CcHer/Wj6Mgxg6wac99cAfBDAK4vG/guAR5s/PwrgjzfIjy8A+NctXo8BAB9s/twF4CSAu1u9JhE/WromaBSw6Gz+nAfwHBrVs74D4BPN8f8B4PdXc56NuPM/AOC0u5/1Rp3/xwE8vAF+bBju/iyAsSXDD6NRBRloUTVk4kfLcfcr7v5i8+cpNCpF7USL1yTiR0vxButeMXsjgn8ngAuLft/Iyr8O4Cdm9oKZPbJBPrxJv7tfARoXIQDeo3v9+ayZHWu+LVj3tx+LMbO9aBSPeQ4buCZL/ABavCatqJi9EcEfqsm1UXrjg+7+QQC/DeAPzOzXNsiP24mvATiARoOWKwC+1KoTm1kngCcAfM7dJ1t13hX40fI18VVUzF4pGxH8FwEMLvqdVv5db9z9cvP7KIDvY2PLko2Y2QAANL+PboQT7j7SvPDqAL6OFq2JmeXRCLhvufv3msMtX5OQHxu1Js1zv+OK2StlI4L/eQAHmzuXBQCfAPBkq50wsw4z63rzZwAfAcCb+K0/T6JRBRnYwGrIbwZbk4+jBWtiZoZGAdgT7v7lRaaWrgnzo9Vr0rKK2a3awVyym/lRNHZSzwD4dxvkw340lIaXALzaSj8AfBuNl48VNF4JfQbAFgBPAzjV/N67QX78OYCXARxDI/gGWuDHP0DjJewxAEebXx9t9ZpE/GjpmgB4PxoVsY+h8Y/m3y+6Zn8B4DSA/w2guJrz6OO9QiSKPuEnRKIo+IVIFAW/EImi4BciURT8QiSKgl+IRFHwC5Eo/w+OGHeiFzRYAAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs, classes = train_dataset[0]\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "imshow(out, title=idx_to_class[classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor from recitation 6 and https://github.com/hysts/pytorch_resnet/blob/master/resnet.py\n",
    "\n",
    "class BottleneckBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "        self.expansion = 4\n",
    "        bottleneck_channels = out_channels // self.expansion\n",
    "        self.in_channels, self.out_channels = in_channels, out_channels\n",
    "        self.conv1 = nn.Conv2d(in_channels,bottleneck_channels,kernel_size=1,stride=1,padding=0,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(bottleneck_channels)\n",
    "        self.conv2 = nn.Conv2d(bottleneck_channels,bottleneck_channels,kernel_size=3,stride=stride,padding=1,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n",
    "        self.conv3 = nn.Conv2d(bottleneck_channels,out_channels,kernel_size=1,stride=1,padding=0,bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride,padding=0,bias=False),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
    "        y = F.relu(self.bn2(self.conv2(y)), inplace=True)\n",
    "        y = self.bn3(self.conv3(y))\n",
    "        y += self.shortcut(x)\n",
    "        y = F.relu(y, inplace=True)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor from recitation 6 and https://github.com/hysts/pytorch_resnet/blob/master/resnet.py\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, num_feats, num_classes):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        self.layers.append(nn.Conv2d(in_channels=num_feats, out_channels=64, kernel_size=5, padding = 2, stride=1, bias=False))\n",
    "        self.layers.append(nn.BatchNorm2d(64))\n",
    "        self.layers.append(nn.ReLU(inplace=True))\n",
    "        self.layers.append(nn.MaxPool2d(3, stride=2, padding=1))\n",
    "\n",
    "        in_channels = 64\n",
    "        for out_channels, block_count in zip([128,256,512,1024],[2,3,5,3]):\n",
    "            for i in range(block_count):\n",
    "                self.layers.append(BottleneckBlock(in_channels, out_channels))\n",
    "                in_channels = out_channels\n",
    "\n",
    "        self.layers.append(nn.AdaptiveAvgPool2d((1,1)))\n",
    "        self.layers.append(nn.Flatten())\n",
    "        self.layers.append(nn.BatchNorm1d(1024))\n",
    "        self.layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "        self.linear_label = nn.Linear(1024, num_classes, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.layers(x)\n",
    "          \n",
    "        label_output = self.linear_label(features)\n",
    "        #label_output = label_output/torch.norm(self.linear_label.weight, dim=1)\n",
    "        \n",
    "        return features, label_output\n",
    "    \n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "    elif type(m) == nn.BatchNorm2d:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (layers): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): BottleneckBlock(\n",
      "      (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): BottleneckBlock(\n",
      "      (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (6): BottleneckBlock(\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): BottleneckBlock(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (8): BottleneckBlock(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (9): BottleneckBlock(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): BottleneckBlock(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (11): BottleneckBlock(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (12): BottleneckBlock(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (13): BottleneckBlock(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (14): BottleneckBlock(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): BottleneckBlock(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (16): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (17): Flatten()\n",
      "    (18): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "  )\n",
      "  (linear_label): Linear(in_features=1024, out_features=2300, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Network(3, len(train_dataset.classes))\n",
    "model.apply(init_weights)\n",
    "model.cuda()\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, test_loader, task='Classification'):\n",
    "    best_model = None\n",
    "    lowest_val_acc = None\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(10):\n",
    "        avg_loss = 0.0\n",
    "        for batch_num, (feats, labels) in tqdm(enumerate(data_loader)):\n",
    "            feats, labels = feats.cuda(), labels.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(feats)[1]\n",
    "\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            if batch_num % 100 == 99:\n",
    "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/100))\n",
    "                avg_loss = 0.0    \n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "            del loss\n",
    "        scheduler.step()\n",
    "        \n",
    "            \n",
    "        val_loss, val_acc = test_classify(model, test_loader)\n",
    "        #train_loss, train_acc = test_classify(model, data_loader)\n",
    "        print('Val Loss: {:.4f}\\tVal Accuracy: {:.4f}'.format(val_loss, val_acc))\n",
    "        torch.save(model.state_dict(), MODEL_PATH+'Checkpoint/model_%d_%d.pt'%(int(time.time()), epoch+1))\n",
    "        if ((best_model == None) or (val_acc < lowest_acc)):\n",
    "            best_model = model\n",
    "            lowest_acc = val_acc\n",
    "            \n",
    "    torch.save(best_model.state_dict(), MODEL_PATH+'model_%d.pt'%int(time.time()))\n",
    "    return best_model\n",
    "           \n",
    "\n",
    "def test_classify(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    accuracy = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_num, (feats, labels) in tqdm(enumerate(test_loader)):\n",
    "            feats, labels = feats.cuda(), labels.cuda()\n",
    "            outputs = model(feats)[1]\n",
    "\n",
    "            _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
    "            pred_labels = pred_labels.view(-1)\n",
    "\n",
    "            loss = criterion(outputs, labels.long())\n",
    "\n",
    "            accuracy += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "            total += len(labels)\n",
    "            test_loss.extend([loss.item()]*feats.size()[0])\n",
    "            del feats\n",
    "            del labels\n",
    "    model.train()\n",
    "    return np.mean(test_loss), accuracy/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2b0d3aaa7b4a97877d2ff60a85ec02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 100\tAvg-Loss: 6.4390\n",
      "Epoch: 1\tBatch: 200\tAvg-Loss: 6.2597\n",
      "Epoch: 1\tBatch: 300\tAvg-Loss: 6.0549\n",
      "Epoch: 1\tBatch: 400\tAvg-Loss: 5.8940\n",
      "Epoch: 1\tBatch: 500\tAvg-Loss: 5.7199\n",
      "Epoch: 1\tBatch: 600\tAvg-Loss: 5.5639\n",
      "Epoch: 1\tBatch: 700\tAvg-Loss: 5.4209\n",
      "Epoch: 1\tBatch: 800\tAvg-Loss: 5.2822\n",
      "Epoch: 1\tBatch: 900\tAvg-Loss: 5.1538\n",
      "Epoch: 1\tBatch: 1000\tAvg-Loss: 5.0429\n",
      "Epoch: 1\tBatch: 1100\tAvg-Loss: 4.9057\n",
      "Epoch: 1\tBatch: 1200\tAvg-Loss: 4.8180\n",
      "Epoch: 1\tBatch: 1300\tAvg-Loss: 4.6932\n",
      "Epoch: 1\tBatch: 1400\tAvg-Loss: 4.6001\n",
      "Epoch: 1\tBatch: 1500\tAvg-Loss: 4.4641\n",
      "Epoch: 1\tBatch: 1600\tAvg-Loss: 4.3939\n",
      "Epoch: 1\tBatch: 1700\tAvg-Loss: 4.2794\n",
      "Epoch: 1\tBatch: 1800\tAvg-Loss: 4.2213\n",
      "Epoch: 1\tBatch: 1900\tAvg-Loss: 4.1259\n",
      "Epoch: 1\tBatch: 2000\tAvg-Loss: 4.0456\n",
      "Epoch: 1\tBatch: 2100\tAvg-Loss: 3.9828\n",
      "Epoch: 1\tBatch: 2200\tAvg-Loss: 3.9188\n",
      "Epoch: 1\tBatch: 2300\tAvg-Loss: 3.8377\n",
      "Epoch: 1\tBatch: 2400\tAvg-Loss: 3.7650\n",
      "Epoch: 1\tBatch: 2500\tAvg-Loss: 3.7088\n",
      "Epoch: 1\tBatch: 2600\tAvg-Loss: 3.6550\n",
      "Epoch: 1\tBatch: 2700\tAvg-Loss: 3.5849\n",
      "Epoch: 1\tBatch: 2800\tAvg-Loss: 3.5390\n",
      "Epoch: 1\tBatch: 2900\tAvg-Loss: 3.4670\n",
      "Epoch: 1\tBatch: 3000\tAvg-Loss: 3.4547\n",
      "Epoch: 1\tBatch: 3100\tAvg-Loss: 3.3823\n",
      "Epoch: 1\tBatch: 3200\tAvg-Loss: 3.3161\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7453b246a23a4718aeae490148d894bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Loss: 3.8344\tVal Accuracy: 0.2683\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8369317b9a4064837136305050fad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\tBatch: 100\tAvg-Loss: 3.1172\n",
      "Epoch: 2\tBatch: 200\tAvg-Loss: 3.0747\n",
      "Epoch: 2\tBatch: 300\tAvg-Loss: 3.0480\n",
      "Epoch: 2\tBatch: 400\tAvg-Loss: 3.0126\n",
      "Epoch: 2\tBatch: 500\tAvg-Loss: 3.0062\n",
      "Epoch: 2\tBatch: 600\tAvg-Loss: 2.9722\n",
      "Epoch: 2\tBatch: 700\tAvg-Loss: 2.9262\n",
      "Epoch: 2\tBatch: 800\tAvg-Loss: 2.8894\n",
      "Epoch: 2\tBatch: 900\tAvg-Loss: 2.8901\n",
      "Epoch: 2\tBatch: 1000\tAvg-Loss: 2.8407\n",
      "Epoch: 2\tBatch: 1100\tAvg-Loss: 2.8103\n",
      "Epoch: 2\tBatch: 1200\tAvg-Loss: 2.8280\n",
      "Epoch: 2\tBatch: 1300\tAvg-Loss: 2.7845\n",
      "Epoch: 2\tBatch: 1400\tAvg-Loss: 2.7441\n",
      "Epoch: 2\tBatch: 1500\tAvg-Loss: 2.7204\n",
      "Epoch: 2\tBatch: 1600\tAvg-Loss: 2.7060\n",
      "Epoch: 2\tBatch: 1700\tAvg-Loss: 2.6859\n",
      "Epoch: 2\tBatch: 1800\tAvg-Loss: 2.6377\n",
      "Epoch: 2\tBatch: 1900\tAvg-Loss: 2.6233\n",
      "Epoch: 2\tBatch: 2000\tAvg-Loss: 2.6143\n",
      "Epoch: 2\tBatch: 2100\tAvg-Loss: 2.6030\n",
      "Epoch: 2\tBatch: 2200\tAvg-Loss: 2.5763\n",
      "Epoch: 2\tBatch: 2300\tAvg-Loss: 2.5616\n",
      "Epoch: 2\tBatch: 2400\tAvg-Loss: 2.5603\n",
      "Epoch: 2\tBatch: 2500\tAvg-Loss: 2.5440\n",
      "Epoch: 2\tBatch: 2600\tAvg-Loss: 2.5242\n",
      "Epoch: 2\tBatch: 2700\tAvg-Loss: 2.4713\n",
      "Epoch: 2\tBatch: 2800\tAvg-Loss: 2.4518\n",
      "Epoch: 2\tBatch: 2900\tAvg-Loss: 2.4588\n",
      "Epoch: 2\tBatch: 3000\tAvg-Loss: 2.4196\n",
      "Epoch: 2\tBatch: 3100\tAvg-Loss: 2.4476\n",
      "Epoch: 2\tBatch: 3200\tAvg-Loss: 2.3938\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e0db0d6dda4678b53948c9377bbcae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Loss: 2.8419\tVal Accuracy: 0.4228\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334fd2a40f554665858f0a5d6099f93b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\tBatch: 100\tAvg-Loss: 2.1897\n",
      "Epoch: 3\tBatch: 200\tAvg-Loss: 2.1663\n",
      "Epoch: 3\tBatch: 300\tAvg-Loss: 2.1670\n",
      "Epoch: 3\tBatch: 400\tAvg-Loss: 2.1768\n",
      "Epoch: 3\tBatch: 500\tAvg-Loss: 2.1721\n",
      "Epoch: 3\tBatch: 600\tAvg-Loss: 2.1695\n",
      "Epoch: 3\tBatch: 700\tAvg-Loss: 2.1692\n",
      "Epoch: 3\tBatch: 800\tAvg-Loss: 2.1743\n",
      "Epoch: 3\tBatch: 900\tAvg-Loss: 2.1303\n",
      "Epoch: 3\tBatch: 1000\tAvg-Loss: 2.1651\n",
      "Epoch: 3\tBatch: 1100\tAvg-Loss: 2.1359\n",
      "Epoch: 3\tBatch: 1200\tAvg-Loss: 2.1396\n",
      "Epoch: 3\tBatch: 1300\tAvg-Loss: 2.1102\n",
      "Epoch: 3\tBatch: 1400\tAvg-Loss: 2.1083\n",
      "Epoch: 3\tBatch: 1500\tAvg-Loss: 2.1180\n",
      "Epoch: 3\tBatch: 1600\tAvg-Loss: 2.1194\n",
      "Epoch: 3\tBatch: 1700\tAvg-Loss: 2.1364\n",
      "Epoch: 3\tBatch: 1800\tAvg-Loss: 2.1055\n",
      "Epoch: 3\tBatch: 1900\tAvg-Loss: 2.1029\n",
      "Epoch: 3\tBatch: 2000\tAvg-Loss: 2.1080\n",
      "Epoch: 3\tBatch: 2100\tAvg-Loss: 2.1127\n",
      "Epoch: 3\tBatch: 2200\tAvg-Loss: 2.0993\n",
      "Epoch: 3\tBatch: 2300\tAvg-Loss: 2.0682\n",
      "Epoch: 3\tBatch: 2400\tAvg-Loss: 2.0652\n",
      "Epoch: 3\tBatch: 2500\tAvg-Loss: 2.0872\n",
      "Epoch: 3\tBatch: 2600\tAvg-Loss: 2.0474\n",
      "Epoch: 3\tBatch: 2700\tAvg-Loss: 2.0170\n",
      "Epoch: 3\tBatch: 2800\tAvg-Loss: 2.0489\n",
      "Epoch: 3\tBatch: 2900\tAvg-Loss: 2.0204\n",
      "Epoch: 3\tBatch: 3000\tAvg-Loss: 2.0435\n",
      "Epoch: 3\tBatch: 3100\tAvg-Loss: 2.0367\n",
      "Epoch: 3\tBatch: 3200\tAvg-Loss: 2.0485\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52dae2d1f720469a8b3acc1db537852d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Loss: 2.4132\tVal Accuracy: 0.5074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132630b7e0724d4181ffd072d56601e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\tBatch: 100\tAvg-Loss: 1.7934\n",
      "Epoch: 4\tBatch: 200\tAvg-Loss: 1.7779\n",
      "Epoch: 4\tBatch: 300\tAvg-Loss: 1.8064\n",
      "Epoch: 4\tBatch: 400\tAvg-Loss: 1.7844\n",
      "Epoch: 4\tBatch: 500\tAvg-Loss: 1.8085\n",
      "Epoch: 4\tBatch: 600\tAvg-Loss: 1.8043\n",
      "Epoch: 4\tBatch: 700\tAvg-Loss: 1.8110\n",
      "Epoch: 4\tBatch: 800\tAvg-Loss: 1.8054\n",
      "Epoch: 4\tBatch: 900\tAvg-Loss: 1.7978\n",
      "Epoch: 4\tBatch: 1000\tAvg-Loss: 1.7985\n",
      "Epoch: 4\tBatch: 1100\tAvg-Loss: 1.8387\n",
      "Epoch: 4\tBatch: 1200\tAvg-Loss: 1.8357\n",
      "Epoch: 4\tBatch: 1300\tAvg-Loss: 1.8516\n",
      "Epoch: 4\tBatch: 1400\tAvg-Loss: 1.8203\n",
      "Epoch: 4\tBatch: 1500\tAvg-Loss: 1.7898\n",
      "Epoch: 4\tBatch: 1600\tAvg-Loss: 1.8368\n",
      "Epoch: 4\tBatch: 1700\tAvg-Loss: 1.8222\n",
      "Epoch: 4\tBatch: 1800\tAvg-Loss: 1.8213\n",
      "Epoch: 4\tBatch: 1900\tAvg-Loss: 1.8085\n",
      "Epoch: 4\tBatch: 2000\tAvg-Loss: 1.8282\n",
      "Epoch: 4\tBatch: 2100\tAvg-Loss: 1.8139\n",
      "Epoch: 4\tBatch: 2200\tAvg-Loss: 1.8084\n",
      "Epoch: 4\tBatch: 2300\tAvg-Loss: 1.8328\n",
      "Epoch: 4\tBatch: 2400\tAvg-Loss: 1.8138\n",
      "Epoch: 4\tBatch: 2500\tAvg-Loss: 1.7993\n",
      "Epoch: 4\tBatch: 2600\tAvg-Loss: 1.8258\n",
      "Epoch: 4\tBatch: 2700\tAvg-Loss: 1.8201\n",
      "Epoch: 4\tBatch: 2800\tAvg-Loss: 1.7958\n",
      "Epoch: 4\tBatch: 2900\tAvg-Loss: 1.8215\n",
      "Epoch: 4\tBatch: 3000\tAvg-Loss: 1.7875\n",
      "Epoch: 4\tBatch: 3100\tAvg-Loss: 1.8035\n",
      "Epoch: 4\tBatch: 3200\tAvg-Loss: 1.7946\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7244ce3acb60460a8c9874232e4351e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Loss: 2.3241\tVal Accuracy: 0.5237\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "528d6305ca7f487daf06da8822782621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\tBatch: 100\tAvg-Loss: 1.5727\n",
      "Epoch: 5\tBatch: 200\tAvg-Loss: 1.5555\n",
      "Epoch: 5\tBatch: 300\tAvg-Loss: 1.5665\n",
      "Epoch: 5\tBatch: 400\tAvg-Loss: 1.5918\n",
      "Epoch: 5\tBatch: 500\tAvg-Loss: 1.6077\n",
      "Epoch: 5\tBatch: 600\tAvg-Loss: 1.5979\n",
      "Epoch: 5\tBatch: 700\tAvg-Loss: 1.5926\n",
      "Epoch: 5\tBatch: 800\tAvg-Loss: 1.6021\n",
      "Epoch: 5\tBatch: 900\tAvg-Loss: 1.6206\n",
      "Epoch: 5\tBatch: 1000\tAvg-Loss: 1.6000\n",
      "Epoch: 5\tBatch: 1100\tAvg-Loss: 1.6040\n",
      "Epoch: 5\tBatch: 1200\tAvg-Loss: 1.6203\n",
      "Epoch: 5\tBatch: 1300\tAvg-Loss: 1.6566\n",
      "Epoch: 5\tBatch: 1400\tAvg-Loss: 1.6229\n",
      "Epoch: 5\tBatch: 1500\tAvg-Loss: 1.6433\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-a41f5a48d7d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-6e0873cb3c89>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, test_loader, task)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_num\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m99\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_model = train(model, train_dataloader, dev_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6819d882ec314381a885671792a034f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Loss: 2.3366\tVal Accuracy: 0.5133\n"
     ]
    }
   ],
   "source": [
    "#best_model = model\n",
    "best_model = Network(3, len(train_dataset.classes))\n",
    "best_model.load_state_dict(torch.load(MODEL_PATH+'Checkpoint/model_1583538327_4.pt'))\n",
    "best_model.cuda()\n",
    "val_loss, val_acc = test_classify(best_model, dev_dataloader)\n",
    "print('Val Loss: {:.4f}\\tVal Accuracy: {:.4f}'.format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_train(model, data_loader, test_loader, task='Classification'):\n",
    "    best_model = None\n",
    "    lowest_val_acc = None\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(5):\n",
    "        avg_loss = 0.0\n",
    "        for batch_num, (feats, labels) in tqdm(enumerate(data_loader)):\n",
    "            feats, labels = feats.cuda(), labels.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(feats)[1]\n",
    "\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            if batch_num % 100 == 99:\n",
    "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/100))\n",
    "                avg_loss = 0.0    \n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "            del loss\n",
    "            \n",
    "        fine_scheduler.step()\n",
    "        val_loss, val_acc = test_classify(model, test_loader)\n",
    "        print('Val Loss: {:.4f}\\tVal Accuracy: {:.4f}'.\n",
    "                format(val_loss, val_acc))\n",
    "        torch.save(model.state_dict(), MODEL_PATH+'Checkpoint/model_%d_%d.pt'%(int(time.time()), epoch+1))\n",
    "        if ((best_model == None) or (val_acc < lowest_acc)):\n",
    "            best_model = model\n",
    "            lowest_acc = val_acc\n",
    "    torch.save(best_model.state_dict(), MODEL_PATH+'model_%d.pt'%int(time.time()))\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412eb433361346ae9f6a58afb1aedab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model = fine_train(best_model, train_dataloader, dev_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestImageFolder(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.file_list = []\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.jpg'):\n",
    "                    self.file_list.append(file)\n",
    "        self.file_list.sort()\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.file_list[idx])\n",
    "        to_pil = torchvision.transforms.Compose([torchvision.transforms.ToPILImage()])\n",
    "        image = to_pil(plt.imread(img_name))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TestImageFolder(DATA_PATH+'test_classification/medium', \n",
    "                                               transform=data_transforms)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                                             shuffle=False, num_workers=num_workers,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (test_dataset.file_list[0])\n",
    "print (test_dataset[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = test_dataset[0]\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "imshow(out,title=test_dataset.file_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network(3, len(train_dataset.classes))\n",
    "model.load_state_dict(torch.load(MODEL_PATH+'model_1583478712.pt'))\n",
    "model.cuda()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "image_names = test_dataset.file_list\n",
    "test_preds = torch.LongTensor().cuda()\n",
    "with torch.no_grad():\n",
    "    for batch_num, feats in tqdm(enumerate(test_dataloader)):\n",
    "        feats = feats.cuda()\n",
    "        outputs = model(feats)[1]\n",
    "\n",
    "        _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        test_preds = torch.cat((test_preds, pred_labels), dim=0)\n",
    "\n",
    "        del feats\n",
    "        del pred_labels\n",
    "    \n",
    "out_df = pd.DataFrame()\n",
    "out_df['Id'] = image_names\n",
    "out_df['Category'] = test_preds.cpu().numpy()\n",
    "out_df['Category'] = out_df['Category'].map(idx_to_class)\n",
    "out_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = SUBMISSION_PATH+\"submission_%d.csv\"%int(time.time())\n",
    "out_df.to_csv(file_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Network(3, len(train_dataset.classes),4096)\n",
    "model1.load_state_dict(torch.load(MODEL_PATH+'model_1583111170.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
